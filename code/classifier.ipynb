{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.metrics import pairwise_distances\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "import embed_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph(G):\n",
    "    # G = nx.Graph()\n",
    "    # G.add_edge(1,2)\n",
    "    # G.add_edge(2,3)\n",
    "    # for v in G.nodes():\n",
    "    #     G.node[v]['state']='X'\n",
    "    # G.node[1]['state']='Y'\n",
    "    # G.node[2]['state']='Y'\n",
    "\n",
    "    # for n in G.edges_iter():\n",
    "    #     G.edge[n[0]][n[1]]['state']='X'\n",
    "    # G.edge[2][3]['state']='Y'\n",
    "\n",
    "    node_labels = nx.get_node_attributes(G,embed_utils.CLASS_NAME)\n",
    "    node_attr = nx.get_node_attributes(G,embed_utils.SENSATTR)\n",
    "\n",
    "    color_map = []\n",
    "    for node in G:\n",
    "        if node_attr[node] == 0:\n",
    "            color_map.append('blue')\n",
    "        else: \n",
    "            color_map.append('green')   \n",
    "\n",
    "    # pos = nx.spring_layout(G)\n",
    "\n",
    "    # nx.draw(G, pos)\n",
    "    nx.draw(G, node_color=color_map, labels = node_labels)\n",
    "    # nx.draw_networkx_labels(G, pos, labels = node_labels)\n",
    "    # edge_labels = nx.get_edge_attributes(G,'state')\n",
    "    # nx.draw_networkx_edge_labels(G, pos, labels = edge_labels)\n",
    "    # plt.savefig('this.png')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(classifier, test_nodes, embeddings, label_dict):\n",
    "    test_node_embeddings = embeddings[test_nodes]\n",
    "    test_node_labels = [label_dict[node] for node in test_nodes]\n",
    "    pred = classifier.predict(test_node_embeddings)\n",
    "    accuracy = sum(test_node_labels == pred) / len(test_node_labels)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def check_classification_params(nodes, labels, embeddings):\n",
    "    assert nodes == list(range(len(nodes)))\n",
    "    assert len(labels) == len(nodes) == len(embeddings), f\"{len(labels)}, {len(nodes)}, {len(embeddings)}\"\n",
    "\n",
    "\n",
    "def make_classification_model(nodes, labels, embeddings):\n",
    "    \"\"\"\n",
    "    Makes the classification model\n",
    "    :param nodes: should be the nodes as list of consecutive integers\n",
    "    :param labels: should be a list of labels where a value of -1\n",
    "        indicates a missing label\n",
    "    :param embeddings: is the embeddings of all nodes obtained using a\n",
    "        (modified) random walk\n",
    "\n",
    "    :returns: a model with a predict() function that predicts the label\n",
    "        from embeddings\n",
    "    \"\"\"\n",
    "    check_classification_params(nodes, labels, embeddings)\n",
    "\n",
    "    g = np.mean(pairwise_distances(embeddings))\n",
    "    # TODO is this the right kernel?\n",
    "    clf = LabelPropagation(kernel=\"knn\", gamma = g).fit(embeddings, labels)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def run_classification(graph, embeddings, name_ds=\"\"):\n",
    "    # show_graph(graph)\n",
    "\n",
    "    # print(\"Graph info\")\n",
    "    # print(f\"Number of nodes: {len(graph.nodes())}\")\n",
    "    # print(f\"Number of edges: {len(graph.edges())}\")\n",
    "    # print(f\"Length label dict: {len(nx.get_node_attributes(graph, embed_utils.CLASS_NAME))}\")\n",
    "    # print(f\"Length attri dict: {len(nx.get_node_attributes(graph, embed_utils.SENSATTR))}\")\n",
    "    # print(\"Label dict:\", nx.get_node_attributes(graph, embed_utils.CLASS_NAME))\n",
    "    # print(\"Attri dict:\", nx.get_node_attributes(graph, embed_utils.SENSATTR))\n",
    "\n",
    "    # Get labels and attributes of test nodes to other classes\n",
    "    label_dict = deepcopy(nx.get_node_attributes(graph, embed_utils.CLASS_NAME))\n",
    "    attr_dict = deepcopy(nx.get_node_attributes(graph, embed_utils.SENSATTR))\n",
    "    \n",
    "    # Split in to equal sized train and test nodes\n",
    "    nodes = list(graph.nodes())\n",
    "    train_nodes, test_nodes = train_test_split(nodes, test_size=0.5, shuffle=True)\n",
    "    train_nodes = set(train_nodes)\n",
    "\n",
    "    # Get semi-supervised labels\n",
    "    n_unique_labels = len(set(label_dict.values()))\n",
    "    unique_labels = (x for x in np.arange(n_unique_labels+1, len(train_nodes) + n_unique_labels + 2))\n",
    "    semi_supervised_y = [label_dict[node] if node in train_nodes else next(unique_labels) for node in nodes]\n",
    "\n",
    "    # Train the classifier\n",
    "    clf = make_classification_model(nodes, semi_supervised_y, embeddings)\n",
    "\n",
    "    # test_node_embeddings = [emb for i, emb in enumerate(embeddings) if i in test_nodes]\n",
    "    # test_node_labels = [label_dict[node] for node in test_nodes]\n",
    "    # pred = classifier.predict(test_node_embeddings)\n",
    "    # accuracy = sum(test_node_labels == pred) / len(test_node_labels)\n",
    "\n",
    "    # Get test node embeddings, labels and find accuracy on test nodes\n",
    "    c0_nodes = [node for node in test_nodes if attr_dict[node] == 0]\n",
    "    acc_c0 = get_metrics(clf, c0_nodes, embeddings, label_dict) * 100\n",
    "\n",
    "    c1_nodes = [node for node in test_nodes if attr_dict[node] == 1]\n",
    "    acc_c1 = get_metrics(clf, c1_nodes, embeddings, label_dict) * 100\n",
    "\n",
    "    accuracy = get_metrics(clf, test_nodes, embeddings, label_dict) * 100\n",
    "    disparity = np.var([acc_c0, acc_c1])\n",
    "    # print(f\"Accuracy c0: {acc_c0}\")\n",
    "    # print(f\"Accuracy c1: {acc_c1}\")\n",
    "    # print(f\"Disparity: {disparity}\") \n",
    "    # print()\n",
    "\n",
    "    # print(f\"Total accuracy: {accuracy}\")\n",
    "    # print()\n",
    "    # print(f\"Counter training lables: {Counter([label_dict[node] for node in train_nodes]).most_common(3)}\")\n",
    "    # print(f\"Counter real lables: {Counter([label_dict[node] for node in test_nodes]).most_common(3)}\")\n",
    "    # print(\n",
    "    #     f\"Counter prediction: {Counter(clf.predict([emb for i, emb in enumerate(embeddings) if i in test_nodes])).most_common(3)}\"\n",
    "    #     )\n",
    "    # print()\n",
    "\n",
    "    return accuracy, disparity\n",
    "\n",
    "\n",
    "n_runs = 200\n",
    "for dataset, reweight_method, embed_method in [[\"rice\", \"fairwalk\", \"deepwalk\"]]:\n",
    "    # get graph from data\n",
    "    graph = embed_utils.data2graph(dataset)\n",
    "\n",
    "    # get embedding from graph\n",
    "    embed = embed_utils.graph2embed(\n",
    "                                        graph, \n",
    "                                        reweight_method, \n",
    "                                        embed_method\n",
    "                                        )\n",
    "\n",
    "    accs = []\n",
    "    disparities = []\n",
    "    for i in range(n_runs):\n",
    "        acc, disparity = run_classification(graph, embed)\n",
    "        accs.append(acc)\n",
    "        disparities.append(disparity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72.47085201793722, 88.68697375699398)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accs), np.mean(disparities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2873.8166696716326"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(disparities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.065877053630677"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "004704e42d79ced63795043d89add8ae96304b2a7499c0b7ffed044fe8a1d5bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
